{
  "name": "ocr_v0.2",
  "nodes": [
    {
      "parameters": {
        "operation": "toText",
        "sourceProperty": "markdown",
        "options": {
          "fileName": "완성본.md"
        }
      },
      "type": "n8n-nodes-base.convertToFile",
      "typeVersion": 1.1,
      "position": [
        640,
        -180
      ],
      "id": "fad7bf33-aadf-4ed6-8be3-8f9379185517",
      "name": "마크다운변환"
    },
    {
      "parameters": {},
      "type": "n8n-nodes-base.manualTrigger",
      "typeVersion": 1,
      "position": [
        -60,
        -340
      ],
      "id": "3c49e2b6-f606-46ad-8e5a-b69411ce93a8",
      "name": "시작"
    },
    {
      "parameters": {
        "resource": "fileFolder",
        "filter": {
          "folderId": {
            "__rl": true,
            "value": "1wC23q294eSU6C1GhO5fNDa7TKW-Jx6zT",
            "mode": "list",
            "cachedResultName": "ㅊㅅㅂ",
            "cachedResultUrl": "https://drive.google.com/drive/folders/1wC23q294eSU6C1GhO5fNDa7TKW-Jx6zT"
          }
        },
        "options": {}
      },
      "type": "n8n-nodes-base.googleDrive",
      "typeVersion": 3,
      "position": [
        60,
        -340
      ],
      "id": "4f06c7e6-332f-4937-b173-9d03ccb218c1",
      "name": "책이미지폴더",
      "credentials": {
        "googleDriveOAuth2Api": {
          "id": "J8lL23JM7UuZlYns",
          "name": "Google Drive account"
        }
      }
    },
    {
      "parameters": {
        "jsCode": "const items = $input.all();\n\nconst sortedItems = items.sort((a, b) => {\n  return a.json.name.localeCompare(b.json.name);\n});\n\nreturn sortedItems;\n"
      },
      "type": "n8n-nodes-base.code",
      "typeVersion": 2,
      "position": [
        180,
        -340
      ],
      "id": "288f3e46-9718-47bd-9265-efb3df65793a",
      "name": "촬영순정렬"
    },
    {
      "parameters": {
        "model": {
          "__rl": true,
          "value": "gpt-4o-mini",
          "mode": "list",
          "cachedResultName": "gpt-4o-mini"
        },
        "options": {}
      },
      "type": "@n8n/n8n-nodes-langchain.lmChatOpenAi",
      "typeVersion": 1.2,
      "position": [
        180,
        60
      ],
      "id": "d4a07d3d-6a21-488d-a98b-121c1c24ba62",
      "name": "OpenAI Chat Model",
      "credentials": {
        "openAiApi": {
          "id": "yniX2Hcc5N67aUBp",
          "name": "OpenAi account 3"
        }
      },
      "disabled": true
    },
    {
      "parameters": {
        "promptType": "define",
        "text": "data",
        "options": {
          "systemMessage": "- 이미지에 포함된 모든 텍스트를 정확하게 추출해. \n- 추가적인 설명이나 해석 없이, 이미지에서 인식된 텍스트만 출력해.\n- 요약하지 말고 전체 내용을 추출해\n- 단원 (계층)은 마크다운 양식답게 #로 구분하며 정리해줘 (최상단이 ##로 시작)\n- 일반 텍스트의 경우, 단순하게 출력해\n- 코드의 경우 ```\\n{코드}\\n```로 감싸줘",
          "passthroughBinaryImages": true
        }
      },
      "type": "@n8n/n8n-nodes-langchain.agent",
      "typeVersion": 2,
      "position": [
        300,
        -40
      ],
      "id": "8e4777ee-c543-47b6-8b0c-c2059fb0758e",
      "name": "OCR"
    },
    {
      "parameters": {
        "operation": "download",
        "fileId": {
          "__rl": true,
          "value": "={{ $json.id }}",
          "mode": "id"
        },
        "options": {}
      },
      "type": "n8n-nodes-base.googleDrive",
      "typeVersion": 3,
      "position": [
        20,
        -160
      ],
      "id": "9e0ed132-0a5b-40ba-9f2a-acf5cd8610aa",
      "name": "각 페이지 다운로드",
      "credentials": {
        "googleDriveOAuth2Api": {
          "id": "J8lL23JM7UuZlYns",
          "name": "Google Drive account"
        }
      }
    },
    {
      "parameters": {
        "name": "완성본.md",
        "driveId": {
          "__rl": true,
          "value": "My Drive",
          "mode": "list",
          "cachedResultName": "My Drive",
          "cachedResultUrl": "https://drive.google.com/drive/my-drive"
        },
        "folderId": {
          "__rl": true,
          "value": "1qw0r4RYzzvByEcEWE-T8k2BCJXm-mqfr",
          "mode": "id"
        },
        "options": {}
      },
      "type": "n8n-nodes-base.googleDrive",
      "typeVersion": 3,
      "position": [
        760,
        -180
      ],
      "id": "455a1e64-abaa-4460-b81f-b353f657efc5",
      "name": "마크다운 완성",
      "credentials": {
        "googleDriveOAuth2Api": {
          "id": "J8lL23JM7UuZlYns",
          "name": "Google Drive account"
        }
      }
    },
    {
      "parameters": {
        "options": {}
      },
      "type": "n8n-nodes-base.splitInBatches",
      "typeVersion": 3,
      "position": [
        180,
        -160
      ],
      "id": "64fb7184-3a47-4c0e-8782-3bb1ebc148f4",
      "name": "반복"
    },
    {
      "parameters": {
        "aggregate": "aggregateAllItemData",
        "destinationFieldName": "allData",
        "options": {}
      },
      "type": "n8n-nodes-base.aggregate",
      "typeVersion": 1,
      "position": [
        400,
        -180
      ],
      "id": "a2030e22-bf7c-4617-bb10-7540691da106",
      "name": "전체페이지 합치기"
    },
    {
      "parameters": {
        "jsCode": "const items = $input.first().json.allData; // Aggregate 노드에서 합쳐진 배열\nlet md = '';\n\nfor (const item of items) {\n  md += `${item.output}\\n\\n`; // 실제 필드명이 'output'임을 확인포맷팅\n}\n\nreturn [{ json: { markdown: md } }];\n"
      },
      "type": "n8n-nodes-base.code",
      "typeVersion": 2,
      "position": [
        520,
        -180
      ],
      "id": "376b069e-d153-416a-bd84-e2e1aee8c658",
      "name": "json|str변환"
    }
  ],
  "pinData": {
    "json|str변환": [
      {
        "json": {
          "markdown": "## 12\n\n빅터 데이터베이스로 확장하기: RAG 구현하기\n\n93년 생인 LLM 관련 기술을 구성하는 요소로, 빅터 데이터베이스가 주목받는 사실을 살펴보고, LLM의 실용성과 프로그램 DB의 활용을 기초로 하면서, 이 장에서는 RAG(Retrieval-Augmented Generation)가 데이터 재작성을 전개하는 방식에 대해 자세히 알아보겠다.\n\n앞서 빅터 데이터베이스가 통합적으로 발전된 내용을 살펴보았다. 이제는 우리가 LLM이나 다른 데이터베이스의 구조와 활용 방안에 대해 기초적인 이해를 돕는 내용을 보겠다. 빅터 데이터베이스의 수용 가능성은 여러 주장에 의해 나타나고 있다. 빅터 데이터베이스는 에러 데이터베이스의 작용 원리에 관하여, 빅터 경향을 수행하는 각기 다른 방법론을 사용하여, 데이터 사이즈가 길어진 경우 KNN(K-Nearest Neighbor) 설명에 의해 작용할 수 있게 된다. 빅터 데이터베이스는 수용 가능성이 다양한 형태에 의해 제시되며, 상대적으로 ANN(Approximate Nearest Neighbor)와 HNSW(Hierarchical Navigable Small World) 알고리즘으로 상세하게 설명할 수 있다.\n\n이 장에서 중점적으로 다룰 내용은 빅터 데이터베이스인 피노콘(pinecone) 의 사용법을 알아보고, 이를 기반으로 대규모 데이터 코어 구축 과정 속의 다양한 방법 및 실습들에 대해 다루겠다.\n\n``` \n!pip install pinecone-client sentence-transformers==2.7.0 datasets==2.19.0 faiss-cpu==1.8.0 transformers openai==1.9.0 -qqq\n``` \n\n설정이 끝났다면 이제 시작해 보자.\n\n## 12.1 벡터 데이터베이스란\n\n벡터 데이터베이스는 벡터의 유사성을 기준으로 사용하는 데이터베이스를 말한다. 103장에서 벡터와 유사성 검색에 대한 개념을 설명하였다. 이러한 벡터 데이터베이스의 처리 및 관리 방법에 대해서는 이번 12.1장에서 다뤄보겠다. 이 벡터 데이터베이스의 활용이 여러 분야에서 이루어지고 있으며, 다양한 응용을 통해 현재도 활발히 발전하고 있다. 데이터베이스 적합성에 대해서도 벡터 데이터를 활용한 비즈니스 관점에서의 의미를 더할 수 있다.\n\n### 그림 12.1 데이터베이스에서의 유사성 검색\n\n이번 절에서는 벡터 데이터베이스가 동작할 때 발생할 수 있는 관련사항을 정리하고, 제일 일반적인 벡터 데이터베이스에 대한 설명을 드리겠다. 이후 벡터 데이터베이스가 가져야 할 요소와 그 아이디어의 발전 방향을 설명한다. \n\n### 12.1.1 타임라인과 벡터 데이터베이스\n\n벡터 데이터베이스의 종류는 다양하며, 기존에는 기본적인 유사성이 있었다. 현재는 이러한 벡터 데이터베이스가 더욱 진화하고 있으며, 다양한 응용 사례가 생겨났다. 그래프 기반의 데이터베이스나 객체지향 데이터베이스와의 통합도 이루어지고 있다.\n\n## 12. 데이터 모델과 특징 추출\n\n달리기 모델 통로 이기 반부를 위해 학습하는 과정에서 데이터의 수치를 추론하는 방안을 한다. 그러나 제시된 언어 수업의 변형 모델이 촬영하는 자료를 받는 표 12.1의 모델로 구성되고 있는 기계들을 주시해야 한다.\n\n![그림 12.3](https://blog.storiedabits.com/2020/10/15/representation-learning-10-for-software-engineers.html)\n\n![그림 12.4](https://blog.storiedabits.com/2020/10/15/representation-learning-10-for-software-engineers.html)\n\n이 모델은 사용하여 가장 희귀에 가까운 데이터 찾을 수 있는데, 앞선 연구 가 따르면 기계 를 다루는 가장 수학적인 능력이다. 데이터 사이의 거리를 측정하는 방대한 자료다. 이러한 데이터 모델에서 활용할 수 있는 다양한 기법들이 존재한다. \n\n## 3. 차원 축소\n\n이 방식은 사용하여 변이 기의 수치 저항이 가장 가깝다 찾을 수 있으며, 일치한 성능을 입증하는 기계적 내용을 제공한다. 데이터에서 구별되는 다양성은 파라미터를 통해 차원을 축소하는 시스템들을 구축할 수 있다. \n\n최근에는 2022년 약 GPT가 실리콘이나 복잡성을 말고 있다. 이 기사는 알찬 시스템 설정을 갖고 있는 데이터 모델은 간섭할 것으로 보인다.\n\n```\n## 12.3 제8장 28일 차 챗봇 이야기\n\n답변 모델을 통해 이기지 못하는 과정에서 데이터의 특징을 추출하는 방법을 다룹니다. 제대급 모델에 수렴할 수 있는지에 대한 데이터의 중요성도 설명합니다. 전체적인 공간에서 존재하는 그늘 12.1의 모호성처럼 같은 일에 대한 가변성이 존재하고 다른 데이터라는 길 위에서 구조를 구축할 수 있다.\n\n그림 12.4 서로 다른 형상의 뱅크 선택\n\n기본적인 데이터는 가변적이고, 다른 데이터는 데이터의 변형에 대한 특성을 이용하여_embedding_편의 사의 기계 저작물에 시각적으로 영향을 줄 수 있다. 데이터는 데이터라는 상위 벡터를 이용하여 여러 작업에 사용하게 된다. \n\n이 빅데이터는 각 데이터셋의 가치가 가장 가변적으로 바뀌는 것을 찾아낼 수 있는데, 왼쪽 엔은 각 엔의 차원에서 다양한 특성을 내포한 데이터 측정 방식이다. 데이터 사이의 거리를 측정하는 데 필요한 데이터 특성, 즉 요약 시각화 또는 지각된 기계를 통해 공명하게 변형된 데이터 특성이 포함되어 있다. 예를 들어 기계 학습은 데이터 자체 정보를 부가하는 기법이 있다. \n\n이런 적용은 특히 2022년의 GPT와 같은 파생 결과를 통해 명확해지며, 다양한 인사이트를 갖고 있으며 데이터 측정 가능성이 존재하고 있다. 이러한 알 수 없는 질의 예를 통해 데이터는 예측을 유도하도록 기여한다. \n```\n\n## 12.1.2 벡터 데이터베이스 지표 파악하기\n\n벡터 데이터베이스는 여러 소프트웨어를 활용해 벡터를 저장하고 검색하는 기능을 갖는다. 다음 표는 다양한 벡터 데이터베이스의 지표를 정리한 것이다.\n\n| 벡터 라이브러리 | 벡터 데이터베이스    | \n|----------------|----------------------|\n| Faiss          | Pinecone             |\n| Annoy          | Weaviate             |\n| NMSLIB         | Milvus               |\n| SCANN          | Qdrant               |\n|                | vespa                |\n|                | ElasticSearch        |\n|                | PostgreSQL           |\n|                | MongoDB             |\n|                | Neo4j                |\n\n벡터 데이터베이스는 각각의 지표에서 차이가 무엇인가? 벡터 라이브러리는 벡터를 저장하고 검토하는 데 도움을 주며, 데이터베이스의 목적은 삭제 및 관리의 간소화에 있다. 또한 각 벡터 데이터베이스는 벡터에 대한 검색 기능을 제공한다. \n\n또한, 벡터 데이터베이스가 도입된 경우 그럼 12.7절을 통해 가지는 경향을 살펴볼 수 있다. 크게 벡터 데이터베이스의 모듈이 서로 다른 하드웨어로 이루어질 수 있음을 시사한다. \n\n그림 12.6 벡터 데이터베이스 관련 소프트웨어의 개요\n- Chroma\n- vespa\n- marq\n- drant\n- LanceDB\n- Milvus\n\n- Weaviate\n- Pinecone\n- OpenSearch\n- ClickHouse\n- PostgreSQL\n- redis\n- ROCKSET\n- SingleStore\n\n## 12.2 벡터 데이터베이스 작동 원리\n\n벡터 데이터베이스는 벡터의 기원을 계산하여 유사한 벡터를 찾는 데 사용된다. 벡터 사이의 기원이 계산돼야 벡터를 찾는 기본 개념인 KNN(Nearest Neighbor)으로 정리를 도와준다. 벡터 데이터베이스는 KNN 점검 알고리즘의 변형을 사용하여 처리하는데, 여기서는 ANN(Approximate Nearest Neighbor) 알고리즘 중 하나인 HNSW를 소개한다.\n\n### 12.2.1 KNN 검점과 활용\n\nKNN 알고리즘은 벡터의 가장 가까운 스스로 벡터를 찾는 방법을 설명한다. KNN은 여러 종류의 벡터를 조사하기 때문에 즉각적인 응답 속도를 보장할 수 있다. 하지만 실시간적인 관점에서는 어떤 시간이 필요하므로 정밀함 또한 중요한데, 이때 KNN 알고리즘은 경우에 따라 세부적인 정보가 반영될 수 있다.\n\n## 예제 12.1 데이터 다운로드\n\n```python\n!wget ftp://ftp.ri.cmu.edu/local/language/texmex/corpus/sift.tar.gz\n!tar -xvf sift.tar.gz\n!mv sift/* .\n```\n\n다음으로, 예제 12.2를 실행해 메모리를 확인한 후 사용한 메모리를 기록하는 방법에 대한 예제를 살펴보자. `get_memory_usage_mb()` 함수는 현재 프로세서가 사용하는 메모리의 양을 반환한다. (datas가 포함된) `get_queries`, `get_database`, `get_groundtruth` 메서드는 사용해 볼 데이터 파일을 가져온다.\n\n```python\nimport psutil\n\ndef get_memory_usage_mb():\n    process = psutil.Process()\n    memory_info = process.memory_info()\n    return memory_info.rss / (1024 * 1024)\n\nimport time\nfrom faiss.contrib.datasets import DatasetSIFT1M\n\nds = DatasetSIFT1M()\n\nxb = ds.get_database()\ngt = ds.get_groundtruth()\n```\n\n이제 결과가 정확히 예 12.3과 같이 데이터베이스 200,000개 항목에 대한 1,000,000개 쿼리로 구성된 복잡한 문제에 대해 사용되는 시간이 이 예사이에서 요약되었다. 결과 시간도 600,000개 항목에 대한 쿼리로 원래 사이즈에 비례하여 다양한 결과를 보였다. 데이터베이스의 모양은 아래 코드와 함께 진행한다.\n\n```python\nk = 1\nD, I = xx.shape\nng = 1000\n\nfor i in range(1, 10, 2):\n    start_memory = get_memory_usage_mb()\n    start_indexing = time.time()\n    index = faiss.IndexFlatL2(D)\n    index.add(xb[:i * 100000])\n    end_indexing = time.time()\n    \n    t = time.time()\n    D, I = index.search(xb, k)\n    t1 = time.time()\n    print(f\"{end_indexing - start_indexing} ms {((t1 - t) * 1000 / ng):.3f} ms\")\n```\n\n| 데이터베이스 크기 | 메모리 사용 (MB) | 전체 시간 (ms) |\n|--------------------|-------------------|-----------------|\n| 200,000            | 97.950            | 1,487           |\n| 400,000            | 97.875            | 2,236           |\n| 600,000            | 133.872           | 3,006           |\n| 800,000            | 178.250           | 4,265           |\n| 1,000,000          | 200.000           | 5,205           |\n\n## 12.2.2 ANN 적설란\n\n근사 검색은 이른바 Approximate Nearest Neighbor (이하 ANN) 전략으로, 대량의 데이터베이스에서 주어진 질의와 가장 유사한 후보를 탐색하는 것을 의미한다. 최근의 연구는 ANN 전략을 사용하는 알고리즘이 많이 개발되었으며, 실전 응용 사례는 데이터가 크고 고차원일 때 나타나는 부정확성을 해결하는 데 의미가 있다고 할 수 있다.  \n\nKNN 검색이 발생하기 위한 정렬 정보는 여러 가지로 이해될 수 있다. 특히, KNN의 검색 결과를 특정한 ANN 알고리즘을 통해 곱할 수 있는 구조로 표현하는 것도 가능하다. 이때 이를 고려하면 IVF(Inverted File Index), HNSW(Hierarchical Navigable Small World) 등이 있다.\n\n하나의 생각으로는, ANN 검색 과정은 개별 데이터의 템플릿을 다루는 것으로 설정할 수 있다. 이러한 구조는 작지만 유용한 인덱스, 즉 ANN 알고리즘의 가장 대표적인 데이터 세트를 사용하는 것을 목표로 한다. 여러 차원으로 진행되는 데이터 패턴 역시 수용 가능한 구조로 데이터에서 적설 형식적으로 나타내기도 한다. \n\nHNSW는 특히 효과적인 검색을 위한 그래프 기반 구조로, 데이터의 그래프를 생성하여 특정 유형의 검색을 앙간적으로 수행한다. 이는 전통적인 ANN 검색 알고리즘에서 자주 발생하기도 한다. 이는 일반적인 노드 기반 구조로 연결된 데이터의 경우가 포함된다.  \n\nANN 검색의 일반적인 방식은 KNN과 같은 기본적인 방법을 판별하는 것이다. 특정한 데이터를 반환할 때 고려하는 것이 가장 일반적이다. \n\n다음의 ANN 검색 방식에서는 고차원인 그래프와 구조를 통해 검색하는 방식이 사용된다. 일반적으로 HNSW를 반영하여 데이터를 배포하는 방식이 가장 일반적으로 사용된다. HNSW의 원리가 실제 상수입니다. HNSW가 나타나는 데이터 구조는 리프 노드에 대한 이해가 필요하다. \n\n## 12.2.3 탐색 가능한 작은 세계(NSW)\n\nHNSW는 데이터 검색을 위한 고도로 구성된 데이터 구조로서 간주된다. 이러한 구조는 그래프 내의 여러 프로세스에 비례하여 나타난다. 이러한 방식으로 작동되는 방법이 발생하며, 이는 그래프와 연결된 데이터가 된다. 일반적으로 하나의 노드에서 다른 노드 사이에 발생할 수 있는 과정이나 다양한 검색의 전이.\n\n## 12.9 그래프의 형태\n\n탕수 가능성을 살린 세본, 완전한 그래프(그림 12.10)와 완전한 그래프의 하위 집합(그림 12.10b)을 나타낸다. 그래프가 작용하는 연산의 방법이 주어졌을 때, 그래프의 형태를 바꿔서 연산이 없어진 상태로 도는데, 완전한 그래프에 연산이 없어졌을 경우, 모든 정점이 노드인 형태를 취하게 된다. \n\n그림 12.9 그래프의 형태 설명 (연구소 제안 2021년)\n\n### (a) 완전\n### (b) 작은 서브\n### (c) 격자형 접선\n\n이하의 두 가지 외에 그림 12.109 그래프 형태 설명한 것은 모두 정의 되지 않았다. 한편 그림 12.10과 그림 12.11에서 주장한 것과 같이, 규칙적으로 연결된 경우와 같은 형태의 연산을 표현하는 그래프의 형태에 대해 이야기를 나누면, 규칙적인 연결의 형태가 없어 보이는 경우가 발생할 수 있다는 점에 대해 기둥 기초적으로 제시하였다.\n\n## 12.2.4 개요 구조\n\n그림 12.1과 같은 여러 리스트가 있다고 하자. 연결 리스트는 많은 유형으로자유롭고, 4억으로 리스트를 추가하며 시각을 연관하는 주석 삭제로 서정적으로 용이한 데이터를 기프트하는 것으로 프로세서의 고밀도 컨트롤이 생성되지 않는다. 하지만 반영할 때는 이 데이터가 지속적으로 사용되고 이 데이터를 가지고 매핑해서합법이 명확할 수 있다.\n\n시작\n\n```\n3 → 1 → 4 → 10 → None\n```\n\n그래서 어떻게 생각해 볼 수 있을까? 메타 그래프는 적절히 아는 데이터가 있다는 것과 있다. 그렇지만 12.15와 같은 방법들은 픽스된 레코드들을 통해 주의 준비가 있어 보인다. \n\n이제 구조를 이용하여, 중요한 데이터가 작으면서 구조는 부호를 다룰 수 있다. 그러나 템 플랫폼에 의하면 점이나 조질의 적절한 수준이 줄어들 것이다.\n\n```\n<그래프>\n```\n\n---\n\n이상으로 링크드 리스트에 대한 연구 및 주요 질문들이 다루어졌다.\n\n## 12.3 실험: HNSW 인덱스의 핵심 파라미터 이해하기\n\nHNSW 인덱스의 성능에 영향을 미치는 핵심 요소, 즉 주어진 그래프의 크기와 **M** 파라미터를 변경하여 성능을 평가하는 방법을 설명한다. 각 파라미터가 애플리케이션에 미치는 영향에 대해 설명할 것이다.\n\n### 12.3.1 파라미터 m 이해하기\n\nHNSW의 파라미터 **m**은 선택한 경로에 영향을 미치는 간섭의 정도, 즉 이웃의 수를 결정한다. 이 파라미터는 점의 가장 가까운 이웃을 검색할 때의 성능에 영향을 미친다. \n\n예를 들어, **m**이 16일 때, 인덱스는 16개의 이웃을 고려한다. **m**이 커질수록 HNSW의 성능이 향상되지만, 메모리 사용량이 기하급수적으로 증가한다.\n\n```\nimport numpy as np\n\nd = xx.shape[1]\nmxq = 1000\n\nfor m in [16, 32, 64]:\n    time.sleep()\n    start_index = get_memory_usage_mb()\n    index_add(m)\n    end_index = time.time()\n    print(f\"m 값: {m}, 메모리 사용량: {end_memory - start_memory}\")\n\n    t = time.time()\n    recall_at_k = d_index.search(XX, k)\n    t = time.time()\n    print(f\"recall_at_k: {recall_at_k}, {query}과 {recall_at_k:1.3f}\")\n```\n\n위의 코드를 실행하면 실험 결과를 정량화할 수 있다. 예를 들어, 표 12.4에서는 m 값에 따른 평균 정확도와 지연 시간(ms)을 비교한 결과를 보여준다. HNSW 인덱스의 성능이 m 값에 따라 어떻게 변하는지 확실히 나타난다.\n\n## 12.3.2 파라미터 ef_construction 이해하기\n\n인덱스를 새로 생성하는 과정에서 유사성 검색에 추가한 파라미터가 가장 큰 가변, 즉 ef_construction입니다. 이 ef_construction의 기본적인 기능은 데이터의 효과적인 인덱싱을 돕는 것입니다. ef_construction의 값이 클수록 데이터의 효과적인 인덱싱이 높아지며, 성능향상에 기여합니다. 따라서 ef_construction의 적절한 값을 설정하기 위해 실험을 진행합니다.\n\n다음 예시는 ef_construction의 값으로 40, 80, 160, 320을 사용하는 것에 대한 실험 결과를 보여줍니다.\n\n```python\n# 예제 12.5: ef_construction의 활용 확인\n\nx1 = x.shape[1]\nmq = 100\nxq = x1[mq]\n\nfor ef_construction in [40, 80, 160, 320]:\n    index = faiss.IndexHNSWFlat(d, ef_construction)\n    index.hnsw.efConstruction = ef_construction\n    time_start = time.time()\n    start_memory = get_memory_usage_mb()\n    index.add(x)\n    end_memory = get_memory_usage_mb()\n    end_time = time.time()\n```\n\n## 12.3 ef_construction 변형 방법과 성능 결과\n\n| ef_construction | 메모리 사용량 (MB) | 전체 시간 (ms) | 검색 시간 (ms) |\n|-----------------|---------------------|----------------|-----------------|\n| 40              | 783.0               | 348.8          | 0.965           |\n| 80              | 786.0               | 289.0          | 0.598           |\n| 160             | 786.3               | 861.2          | 0.147           |\n| 320             | 786.4               | 1681.1         | 0.957           |\n\n## 12.4 실습: 파일로서 벡터 검색 구현하기\n\n예제 12.6에서는 index의 설정이 `ef_search`에 16, 32, 64, 128씩 설정하며, 다음과 같은 검색 시작 지점을 설정합니다.\n\n```python\nfor ef_search in [16, 32, 64, 128]:\n    index.hnsw.set_ef_search(ef_search)\n    D_1 = index.search(xa, k)\n    t_1 = time.time()\n```\n\nrecall_at_k_1 = np.equal(gnn[:, -1],).sum() / float(n)\nprint(f'Recall @1: {100.0 * n / query, recall_at_k_1:.3f}')\n\n위의 코드는 후속 속도를 정량화한 예제 12.4절, `ef_search` 값 32 이상으로 설정하면 정확률이 0.97 이상을 달성한다. 적절 시간은 1ms 미만이고 KNN의 경우가 전체적으로 7.6ms가 걸린 것에 비해 10배 이상 빠르다.\n\n### 12.4.1 파일은 클라이언트 사용법\n\n클라이언트를 사용하기 위해서는 계정과 API 키가 필요하다. 개정된 사이트는 [https://www.pinecone.io](https://www.pinecone.io)에서 계정을 생성할 수 있다. 생성 후에는 프로그램에서 이 API Keys를 사용하여 귀하의 API 키를 확인할 수 있다.\n\n```python\nfrom pinecone import Pinecone, ServerlessSpec\npinecone_api_key = \"당신의 API 키\"\n```\n\n#### 12.7 기본 API 인터페이스\n\n다음과 같이 데이터를 저장하고 검색하는 엔드포인트를 설정한다. 엔드포인트와 함께 생성할 수 있는 모델을 12.7절에서 제공한다.\n\n## 12.1 데이터 삽입\n\n```python\nfrom datasets import load_dataset\nfrom sentence_transformers import SentenceTransformer\n\n# 모델 불러오기\nsentence_model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')\n\n# 데이터셋 불러오기\nklue_dp_train = load_dataset('klue', 'klue_dp_train')\n\nembedddings = sentence_model.encode(klue_dp_train['train']['sentence'])\n```\n\n## 12.2 데이터 삽입 및 쿼리 방법\n\n```python\n# PINECONE 기본 데이터 삽입\nembedddings = embedddings.tolist()\ninsert_data = []\n\nfor idx, embedding in enumerate(embedddings):\n    insert_data.append({'id': str(idx), 'values': embedding, 'metadata': {'text': klue_dp_train['train']['sentence'][idx]}})\n```\n\n## 12.1 쿼리 응답\n\n```python\nquery_response = index.query(\n    namespace='llm-book-sub',  # 쿼리 네임스페이스\n    top_k=10,                  # 상위 10개의 결과를 반환\n    include_values=True,\n    include_metadata=True,\n)\n\nquery_response\n```\n\n## 12.2 데이터 업데이트 및 삭제\n\n```python\nnew_embedding = sentence_model.encode(new_text).tolist()\nupdate_response = index.update(\n    id='7ae_BAl_1',\n    values=new_embedding,\n)\n```\n\n## 12.4 라이브러리에서 벡터 데이터베이스 변경하기\n\n93에서 라이브러리로서 RAG를 구현하면서 라이브러리의 기본 벡터 데이터베이스를 사용했다. 라이브러리가 고안된 것은 두 가지 이유 때문이며, 이예양 설명하기를 원한다. 각 텍스트 데이터베이스에 접근하여 다양한 기능을 지원하고, 또한 이 예제에서는 필수적으로 사용할 기본 기사를 제공하고 있다. \n\n### 예시: PineconeVectorStore와 StorageContext 사용\n\n```python\nfrom pinecone import pinecone\npc = Pinecone(key=pinecone_api_key)\npc.create_index(\n    \"quickstart\", dimensions=1536, metric=\"euclidean\", spec=ServerlessSpec(\"asm\", \"as-test-1\")\n)\npinecone_index = pc.Index(\"quickstart\")\n```\n\n### 12.5 실습: 파인콘을 활용해 멀티모달 검색 구현하기\n\n12.4에서 설명한 파인콘 사용을 알아보았고, 최근 RAG에 대해 데이터베이스를 위한 구조를 확립할 필요가 보인다. 12.1에서 설명한 구조를 포함하여, 적절한 타입의 데이터셋이 필요하게 된다. 예를 들어, 지정된 이점이 있는 이미지 데이터셋을 통해서 사용가능성이, 이와 관련된 매우 중요한 정보를 제공할 수 있다.\n\n### 12.5.1 데이터셋\n\n이런 적절한 데이터셋을 프로그램과 스토리에 대한 Diffusion 이론을 통해 구축한다. DiffusionDB는 huggingface.co/datasets/poloclub/diffusionbd/ 데이터셋 사용을 지원한다. DiffusionDB: 2000개의 이미지가 있으며, DiffusionDB 2M의 1400개 버전, 그리고 DiffusionDB Large버전이 1000개를 다운로드 시킬 수 있다. \n\n### 예제 12.5.4 데이터셋 다운로드 \n\n```python\nfrom datasets import load_dataset\ndataset = load_dataset(\"poloclub/diffusiondb\", \"2m_first_ik\", split='train')\n```\n\n## 12.5.2 심층 호흡  \n면이점 검사을 통해 적합한 심층 호흡은 그림 12.19와 같다. 사차 모양의 토끼가 있고 공작 같은 긴 부채 꼬리를 가지고 있다.  \n![그림 12.18: 심층 데이터셋에서 생성한 이미지](https://huggingface.co/datasets/poloclub/dall-e-2)  \n\n## 12.5.3 GPT-4로 이미지 생성하기  \n이번 예제는 GPT-4에 의한 이미지를 입력하고 이 이미지에 대해 실행하도록 지시하며 세 가지 프로포트에서 생성된 형상 이미지를 보여준다. GPT-4의 진행기에 의해 이미지 생성이 저해된 이후 다양한 GPT-4에 대한 예시인 12.15에서 사용자 지침인 base64로 인코딩된 다음의 generate.description을 사용하면 GPT-4로 이미지의 형상 프로포트와 연결한 다음, 더 자세한 사항은 OpenAI의 GPT-4 포트 목록을 참조할 수 있다.\n\n## 12.5.4 프롬프트 저장\n\n예제 12.17의 코드는 OpenAI API를 사용한 파이썬 코드로 OpenAI API 키를 초기화하는 방법을 보여준다. Pinecone을 사용하여 벡터를 제공하는 방식에 대해 설명할 수 있다.\n\n```python\nimport os\nfrom openai import OpenAI\nfrom pinecone import Pinecone, ServerlessSpec\n\npinecone_api_key = os.getenv(\"PINECONE_API_KEY\")  # 자신의 Pinecone API 키 입력\nopenai_api_key = os.getenv(\"OPENAI_API_KEY\")  # 자신의 OpenAI API 키 입력\n\npc = Pinecone(pinecone_api_key=pinecone_api_key)  # Pinecone 초기화\n```\n\n예제 12.15에서 정의한 함수를 활용하여 예제 12.16의 이미지를 생성한다. 결과를 확인하면 이미지의 동작에 대한 설명을 생성할 수 있다.\n\n```python\nimport requests\nimport base64\n\ndef make_b64(original_image):\n    buffered = BytesIO()\n    image.save(buffered, format=\"JPEG\")\n    img_str = base64.b64encode(buffered.getvalue()).decode('utf-8')\n    return img_str\n\ndef generate_description_from_image_gpt4(prompt, image64):\n    headers = {\n        \"Content-Type\": \"application/json\",\n        \"Authorization\": f\"Bearer {client.api_key}\"\n    }\n\n    payload = {\n        \"model\": \"gpt-4\",\n        \"messages\": [\n            {\n                \"role\": \"user\",\n                \"content\": {\n                    \"type\": \"text\",\n                    \"text\": prompt\n                },\n                {\n                    \"type\": \"image_url\",\n                    \"image_url\": {\n                        \"url\": f'data:image/jpeg;base64,{image64}'\n                    }\n                }\n            ],\n        ],\n        \"max_tokens\": 300\n    }\n\n    response_oa = requests.post('https://api.openai.com/v1/chat/completions', headers=headers, json=payload)\n    result = response_oa.json()['choices'][0]['message']['content']\n    return result\n```\n\n## 텍스트 추출\n\n```\nos.environ[\"OPENAI_API_KEY\"] = openai_api_key\nclient = OpenAI()\n\n다음은 예제 12.8을 통해 인덱스를 생성하고 이미 생성된 인덱스를 활용하는 방법을 설명한다. CLIP 모델을 실행할 경우 각 파라미터를 사용하는 형태를 설정해야 하며, 사용하는 메모리의 최적화와 관련해 512차원 파일 내 빈 인덱스를 생성하도록 설정한다. 만약에 생성된 인덱스는 재사용할 수 있도록 하여야 한다.\n\n# 예제 12.8 인덱스 생성\n\nprint(pc.list_indexes())\nindex_name = \"llm-multimodal\"\ntry:\n    pc.create_index(\n        name=index_name,\n        dimension=512,\n        metric=\"cosine\",\n        spec={\"server_spec\":\n                {\"aws\": \"eu-west-1\",\n            }}\n        )\n    print(pc.list_indexes())\nexcept:\n    print(\"Index already exists\")\nindex = pc.index[index_name]\n\n이제 프로토타입 데이터를 테스트하며 변환한 포맷을 확인하고 변환을 수행한다. 이는 사용자에 OpenAI의 openai/clip-vit-base-patch32 모델을 사용하고 CLIP 모델을 실행하기 위해서 144.0에 적합하게 재설계된 데이터세트를 활용할 수 있도록 한다. CLIP 모델에 대한 기술 사양을 확인할 수 있으며, 예제 12.9에서는 테스트 데이터 프로토타입을 확인하고 프로토타입을 테스트하는 방식으로 변환한다. 다루는 것은 Hugging Face의 openai/clip-vit-base-patch32 기본 기술에 대한 맞춤형으로 설정된다.\n\n# 예제 12.9 사용자 생성 임베딩 빠른 데이터에 저장\n\nimport torch\nfrom torch.utils.data import DataLoader\nfrom transformers import AutoTokenizer, CLIPTextModelWithProjection\n\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n\ntext_model = CLIPTextModelWithProjection.from_pretrained(\"openai/clip-vit-base-patch32\")\ntokenizer = AutoTokenizer.from_pretrained(\"openai/clip-vit-base-patch32\")\n\ntokens = tokenizer(dataset['prompt'], padding=True, return_tensors=\"pt\", truncation=True)\nbatch_size = 16\ntext_embs = []\n\nfor start_idx in range(0, len(dataset), batch_size):\n    with torch.no_grad():\n        outputs = text_model(\n            input_ids = tokens['input_ids'][start_idx:start_idx + batch_size],\n            attention_mask = tokens['attention_mask'][start_idx:start_idx + batch_size],\n        )\n        text_emb = outputs.last_hidden_state\n        text_embs.append(text_emb.cpu().numpy())\n\ntext_embs = np.concatenate(text_embs, axis=0)  # (1000, 512)\n\n다음으로 예제 12.20을 사용해 생성된 임베딩 데이터를 빠른 데이터베이스에 저장한다. 각 데이터의 id, values, metadata 등의 키를 가진 사전 형태로 기록하고 upsert 메서스를 통해 데이터베이스에 파라미터를 저장한다.\n\n# 예제 12.20 데이터베이스에 임베딩 인덱스 저장\n\ninput_data = []\nfor ind_int, prompt in zip(range(0, len(dataset)), text_embs.tolist()):\n    input_data.append(\n        {\n            'id': str(ind_int),\n            'values': emb,\n            'metadata': {\n                \"prompt\": prompt\n            }\n        }\n    )\n```\n\n## 12.5 이미지 검색\n\n이 제안은 이미지와 이미지 임베딩으로 변환하고 해당 이미지 배치로, 파인튜닝에서 유사한 콘텐츠를 검색할 수 있게 한다. 아래 12.2.1 코드에서 이미지와 임베딩을 생성하는 데 있어 이미지의 `original_image`를 이용하여 유사한 프로토타입을 찾는다. 변환된 프로토타입 인덱스를 `searched_idx`에 저장한다.\n\n### 예제 12.2 이미지 임베딩을 위한 기본 코드\n\n```python\nfrom transformers import AutoProcessor, CLIPVisionModelWithProjection\nvision_model = CLIPVisionModelWithProjection.from_pretrained(\"openai/clip-vit-base-patch16\")\nprocessor = AutoProcessor.from_pretrained(\"openai/clip-vit-base-patch16\")\ninputs = processor(images=original_image, return_tensors=\"pt\")\noutputs = vision_model(**inputs)\nimage_embeds = outputs.image_embeds\n\nsearch_results = index.query(\n    vector=image_embeds[0].tolist(),\n    top_k=3,\n    include_metadata=True\n)\nsearched_idx = int(search_results['matches'][0]['id'])\n```\n\n### 12.5.6 DALL-E 3로 이미지 생성\n\n이제 모델의 3개 프로토타입으로 이미지를 생성해 보자. 예제 12.2.3 프로토타입 기반으로 이미지를 생성하는 `generate_image_dalle` 함수를 사용해 이미지를 가져온다.\n\n```python\ngenerated_image = generate_image_dalle(\"text to describe the image\")\n```\n\n생성된 이미지는 `get_generated_image` 함수로 URL을 추출해 보여줄 수 있다.\n\n## 12.5 이미지 검색\n\n이 예제는 이미지를 입력으로 변환하고 해당 이미지를 기반으로 유사한 이미지를 검색하는 방법을 보여준다. 예제 12.2의 코드를 바탕으로 여기서 이미지를 `original_image`에 할당한 후, 변환된 프로세서 인덱스에 searched_idx로 저장한다.\n\n```\nfrom transformers import AutoProcessor, CLIPVisionModelWithProjection\n\nvision_model = CLIPVisionModelWithProjection.from_pretrained(\"openai/vit-base-patch16-224\")\nprocessor = AutoProcessor.from_pretrained(\"openai/vit-base-patch16-224\")\n\ninputs = processor(images=original_image, return_tensors=\"pt\")\n\noutputs = vision_model(**inputs)\nimage_embeds = outputs.image_embeds\n\nsearch_results = index.query(\n    vector=image_embeds[0].tolist(),\n    top_k=3,\n    include_metadata=True\n)\nsearched_idx = int(search_results['matches'][0]['id'])\n```\n\n## 12.5.6 DALL-E 3로 이미지 생성\n\n이 예제에서는 3개의 프롬프트로 이미지를 생성해 보자. 예제 12.2의 프롬프트를 기반으로 이미지를 생성하는 `generate_image_dalle3` 함수를 사용하여 이미지를 가져오고, `generated_image = generate_image_dalle3`를 통해 생성서를 확인한 후, 생성된 이미지의 URL을 추출해 이미지를 저장한다.\n\n## 12.2 이미지 생성을 위한 방법 정리\n\n```python\nfrom PIL import Image\n\ndef generate_image3(prompt):\n    response = ai_client.images.generate(\n        model=\"dall-e-3\",\n        prompt=prompt,\n        size=\"1024x1024\",\n        quality=\"standard\",\n        n=1,\n    )\n    result = response.data[0].url\n    return result\n\ndef get_generated_image(image_url):\n    response = requests.get(image_url).content\n    image_filename = \"gen_img.png\"\n    with open(image_filename, \"wb\") as image_file:\n        image_file.write(response)\n    return Image.open(image_filename)\n```\n\n다음은 예제 12.24의 코드를 사용하여 설명된 이미지를 생성한다. 단서 GPT-4가 생성한 이미지를 설명하도록 하고 그 프로프트로 이미지를 생성한다. \n\n다음은 원 프로프트를 사용하여 이미지를 생성한다. 마찬가지로 이미지 웹링크로 요청하여 프로프트로 다시 이미지를 생성한다.\n\n## 12.25 이미지 출력\n\n```python\nimport matplotlib.pyplot as plt\n\nimages = [original_image, image_dalle.get_described_result]\ntitles = ['(a)', '(b)', '(c)', '(d)']\n\nfig, axes = plt.subplots(1, len(images), figsize=(15, 5))\nfor ax, img, title in zip(axes, images, titles):\n    ax.imshow(img)\n    ax.axis('off')\n    ax.set_title(title)\nplt.tight_layout()\nplt.show()\n```\n\n출처는 그림 12.20과 같다. 다시 한 번 각 이미지를 설명하면 (a)는 원본 이미지이며, (b)는 GPT-4로 설명하도록 하고 그 프로프트로 생성한 이미지이다. (c)는 바텀 배열을 통해 나온 프로프트로 생성한 이미지이다. (d)는 이후 가장 유사해 보이는 랜덤 데이터의 프로프트로 생성할 수 있다.\n\n## 12.6 정리\n\n이번 장에서는 벡터 데이터베이스가 동작하는 배경과 다양한 선택지들에 대해서 살펴보았다. 벡터 데이터베이스는 벡터 인덱스의 기능과 필요성을 설명하는 데 중요한 역할을 하였다. 벡터 인덱스는 상당히 빠른 KNN 검색을 지원할 수 있었고, 벡터 검색의 정확도를 높일 수 있는 방법에 대해 논의되었다.\n\n대표적인 ANN 검색 알고리즘 HNSW는 구조상 그러한 빠른 연결을 제공하는 반면, 그 성능은 주변 제약조건에 의하여 크게 좌우되기 때문에 구조를 설계할 때 신중한 접근이 필요하다. 벡터 데이터베이스의 경우 HNSW와 관련된 알고리즘을 활용하더라도, 벡터 인덱스의 출력이 어떻게 되느냐에 따라 각각이 가지는 영향이 달라질 수 있다.\n\n예로서, 대표적인 벡터 데이터베이스의 선택은 다양한 애플리케이션을 지원하고 있으며, 특히 GPT-40, DALL-E 3 모델 등은 특정한 이미지 생성 작업을 진행하는 데 유용한 새로운 아키텍처를 제공한다. 벡터 데이터베이스의 개발 배경도 비슷한 클립 구조에 맞춰 갱신을 요구하는 경향이 있다.\n\n### 참고 자료\n- 벡터 데이터베이스 추천: https://www.datacamp.com/blog/the-top-5-vector-databases\n- 벡터 검색 방법: https://towardsdatascience.com/ifpqh-hnsw-for-billion-scale-similarity-search-89f2f89d90e2\n- 멀티 모달 RAG: https://leverageai.io/blog/multimodal-rag\n- HNSW 벤치마크: https://github.com/brtholomy/hnsw/blob/master/hnsw.py#55\n- GPT-40 API: https://platform.openai.com/docs/guides/vision\n- DALL-E 3 API: https://cookbook.openai.com/articles/what_is_new_with_dalle_3\n\n"
        }
      }
    ]
  },
  "connections": {
    "마크다운변환": {
      "main": [
        [
          {
            "node": "마크다운 완성",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "시작": {
      "main": [
        [
          {
            "node": "책이미지폴더",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "책이미지폴더": {
      "main": [
        [
          {
            "node": "촬영순정렬",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "촬영순정렬": {
      "main": [
        [
          {
            "node": "각 페이지 다운로드",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "OpenAI Chat Model": {
      "ai_languageModel": [
        [
          {
            "node": "OCR",
            "type": "ai_languageModel",
            "index": 0
          }
        ]
      ]
    },
    "OCR": {
      "main": [
        [
          {
            "node": "반복",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "각 페이지 다운로드": {
      "main": [
        [
          {
            "node": "반복",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "반복": {
      "main": [
        [
          {
            "node": "전체페이지 합치기",
            "type": "main",
            "index": 0
          }
        ],
        [
          {
            "node": "OCR",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "전체페이지 합치기": {
      "main": [
        [
          {
            "node": "json|str변환",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "json|str변환": {
      "main": [
        [
          {
            "node": "마크다운변환",
            "type": "main",
            "index": 0
          }
        ]
      ]
    }
  },
  "active": false,
  "settings": {
    "executionOrder": "v1"
  },
  "versionId": "1ee881d4-3a18-4e0d-b553-2ebfc68e1622",
  "meta": {
    "templateCredsSetupCompleted": true,
    "instanceId": "f59c0de63285a64c131184e7bec79da05901447afbe86aac036ebf3df0ced53d"
  },
  "id": "QhjpwkP9K6Out36H",
  "tags": []
}